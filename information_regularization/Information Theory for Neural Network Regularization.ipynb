{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp information_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import ignite\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss, Accuracy, Fbeta\n",
    "from ignite.contrib.handlers import FastaiLRFinder, ProgressBar\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory for Neural Network Regularization\n",
    "\n",
    "We are going to use CIFAR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "img_size = 32 \n",
    "batch_size = 100 \n",
    "normalization_values = torch.tensor(((0.4914, 0.4822, 0.4465), (1, 1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "cifar_whitening_matrix = np.load(open('data/cifar_Z.npy', 'rb')).astype('float32')\n",
    "cifar_mean = np.load(open('data/cifar_mean.npy', 'rb')).reshape(-1).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLinearTransformation:\n",
    "    def __init__(self, transformation_matrix, transformation_mean):\n",
    "        if transformation_matrix.size(0) != transformation_matrix.size(1):\n",
    "            raise ValueError(\"transformation_matrix should be square. Got \" +\n",
    "                             \"[{} x {}] rectangular matrix.\".format(*transformation_matrix.size()))\n",
    "        self.transformation_matrix = transformation_matrix\n",
    "        self.transformation_mean = transformation_mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (N, C, H, W) to be whitened.\n",
    "        Returns:\n",
    "            Tensor: Transformed image.\n",
    "        \"\"\"\n",
    "        if tensor.size(1) * tensor.size(2) * tensor.size(3) != self.transformation_matrix.size(0):\n",
    "            raise ValueError(\"tensor and transformation matrix have incompatible shape.\" +\n",
    "                             \"[{} x {} x {}] != \".format(*tensor[0].size()) +\n",
    "                             \"{}\".format(self.transformation_matrix.size(0)))\n",
    "        batch = tensor.size(0)\n",
    "\n",
    "        flat_tensor = tensor.view(batch, -1)\n",
    "        transformed_tensor = torch.mm(flat_tensor - self.transformation_mean, self.transformation_matrix)\n",
    "\n",
    "        tensor = transformed_tensor.view(tensor.size())\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitening_transform = BatchLinearTransformation(\n",
    "    torch.tensor(cifar_whitening_matrix).cuda(),\n",
    "    torch.tensor(cifar_mean).cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_cifar_dataloaders(batch_size, num_workers=10):\n",
    "    data_path = 'data/cifar10/'\n",
    "    subsets = ['train', 'test']\n",
    "    datasets = {\n",
    "        subset: torchvision.datasets.CIFAR10(\n",
    "            root='data',\n",
    "            train=subset == 'train',\n",
    "            transform=transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                ]\n",
    "            ),\n",
    "            download=True\n",
    "        ) for subset in subsets \n",
    "    }\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        datasets['train'],\n",
    "        batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        datasets['test'],\n",
    "        batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_dl, test_dl \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "cifar_dl_train, cifar_dl_test = get_cifar_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def to_model_dtype(model, x):\n",
    "    is_cuda = next(model.parameters()).is_cuda\n",
    "    dtype =  next(model.parameters()).dtype\n",
    "    if is_cuda:\n",
    "        x = x.cuda()\n",
    "    if dtype is torch.float16:\n",
    "        x = x.half()\n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mini_batch_size = 2\n",
    "x = torch.tensor(np.ones((sample_mini_batch_size, 3, 32, 32), dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class ConvDropoutBlock(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features_in,\n",
    "            n_features_out,\n",
    "            kernel_size,\n",
    "            activation=nn.ReLU(),\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            dropout_rate=None,\n",
    "            use_information_dropout=False):    \n",
    "        super(ConvDropoutBlock, self).__init__()\n",
    "        self.n_features_in = n_features_in\n",
    "        self.n_features_out = n_features_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        if use_information_dropout:\n",
    "            self.use_dropout = False \n",
    "            self.use_information_dropout = True \n",
    "            conv = nn.Conv2d(n_features_in, n_features_out, kernel_size, padding=padding, stride=stride)\n",
    "            self.dropout_layer = InfoDropout(conv, activation)\n",
    "        elif dropout_rate is not None:\n",
    "            self.use_dropout = True\n",
    "            self.use_information_dropout = False\n",
    "            self.dropout_layer = nn.Dropout2d(dropout_rate)\n",
    "            self.conv = nn.Conv2d(n_features_in, n_features_out, kernel_size, padding=padding, stride=stride)\n",
    "        else:\n",
    "            self.use_dropout = False \n",
    "            self.use_information_dropout = False \n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.use_information_dropout:\n",
    "            X, information_dropout_loss = self.dropout_layer(X)\n",
    "        elif self.use_dropout:\n",
    "            X = self.conv(X)\n",
    "            X = self.dropout_layer(X)\n",
    "            information_dropout_loss = to_model_dtype(self, torch.tensor(0))\n",
    "        else:\n",
    "            information_dropout_loss = to_model_dtype(self, torch.tensor(0))\n",
    "        return X, information_dropout_loss\n",
    "\n",
    "\n",
    "class AllConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_filters=[96, 192],\n",
    "        n_classes=10,\n",
    "        init_dropout_rate=0.2,\n",
    "        dropout_rate=0.5,\n",
    "        use_information_dropout=False,\n",
    "        kernel_size=3,\n",
    "        activation=nn.ReLU(),\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(AllConvNet, self).__init__()\n",
    "        self.init_dropout_rate = init_dropout_rate\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(kernel_size, n_filters[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(n_filters[0]),\n",
    "            activation,\n",
    "            nn.Conv2d(n_filters[0], n_filters[0], kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(n_filters[0]),\n",
    "            activation,\n",
    "            nn.Conv2d(n_filters[0], n_filters[0], kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(n_filters[0]),\n",
    "            activation,\n",
    "        )\n",
    "        self.conv1_dropout = ConvDropoutBlock(n_filters[0], n_filters[0], kernel_size, dropout_rate=dropout_rate, use_information_dropout=use_information_dropout, stride=2)\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(n_filters[0], n_filters[1], kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(n_filters[1]),\n",
    "            activation,\n",
    "            nn.Conv2d(n_filters[1], n_filters[1], kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(n_filters[1]),\n",
    "            activation,\n",
    "        )\n",
    "        self.conv2_dropout = ConvDropoutBlock(n_filters[1], n_filters[1], kernel_size, dropout_rate=dropout_rate, use_information_dropout=use_information_dropout, stride=2, padding=1)\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(n_filters[1], n_filters[1], kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(n_filters[1]),\n",
    "            activation,\n",
    "            nn.Conv2d(n_filters[1], n_filters[1], 1),\n",
    "            nn.BatchNorm2d(n_filters[1]),\n",
    "            activation,\n",
    "            nn.Conv2d(n_filters[1], 10, 1),\n",
    "            nn.BatchNorm2d(10),\n",
    "            activation,\n",
    "        )\n",
    "        self.reshape = torch.nn.Sequential(\n",
    "            nn.AvgPool2d(8),\n",
    "            Flatten(),\n",
    "            nn.Linear(10, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = nn.Dropout2d(self.init_dropout_rate)(X)\n",
    "        X = self.conv1(X)\n",
    "        X, info_loss_1 = self.conv1_dropout(X)\n",
    "        X = self.conv2(X)\n",
    "        X, info_loss_2 = self.conv2_dropout(X)\n",
    "        X = self.conv3(X)\n",
    "        return self.reshape(X), info_loss_1 + info_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class LossSumWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, loss, beta=0.0):\n",
    "        super(LossSumWrapper, self).__init__()\n",
    "        self.loss = loss\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, inputs, target, **kwargs):\n",
    "        input, aux_loss = inputs\n",
    "        loss_value = self.loss(input, target)\n",
    "        return loss_value + self.beta * loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AllConvNet(use_information_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv1_dropout.use_information_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(net.conv1_dropout.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    preds = net(to_model_dtype(net, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[1].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preds[0].numpy().shape == (2, 10)\n",
    "assert preds[1].numpy().shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_loss = LossSumWrapper(torch.nn.CrossEntropyLoss(), beta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_loss_value = wrapped_loss(preds, torch.tensor(np.ones(sample_mini_batch_size, dtype='int')))\n",
    "\n",
    "assert wrapped_loss_value.numpy().shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Regularizing Neural Networks by Penalizing Confident Output Distributions](https://openreview.net/pdf?id=HyhbYrGYe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    x_max = x.max(axis=1).values\n",
    "    return x_max + torch.log(torch.exp(x.T - x_max).sum())\n",
    "\n",
    "\n",
    "def entropy_from_logits(logits, eps=1e-4):\n",
    "    logits_lse = log_sum_exp(logits)\n",
    "    p = F.softmax(logits.T, dim=1)\n",
    "    return - ((logits.T - log_sum_exp(logits)) * p).sum(axis=0)\n",
    "\n",
    "\n",
    "class EntropyPenalizedLogLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta):\n",
    "        super(EntropyPenalizedLogLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, input, target, **kwargs):\n",
    "        cross_entropy = F.cross_entropy(input, target)\n",
    "        return cross_entropy + self.beta * entropy_from_logits(input).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = torch.tensor(np.random.randn(50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert entropy_from_logits(x_t).numpy().shape == (50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epll = EntropyPenalizedLogLoss(beta=0.01)\n",
    "y =torch.tensor(np.ones([50], dtype=int)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert epll(x_t, y).numpy().shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_t = torch.Tensor(\n",
    "    np.random.rand(1, 3, 32, 32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Information Dropout](https://arxiv.org/pdf/1611.01353.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class InfoDropout(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            wrapped_layer,\n",
    "            activation,\n",
    "            max_alpha=0.7,\n",
    "            min_alpha=0.001,\n",
    "        ):\n",
    "        input_dim = wrapped_layer.in_channels\n",
    "        output_dim = wrapped_layer.out_channels\n",
    "        super(InfoDropout, self).__init__()\n",
    "        self.get_alpha = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                input_dim,\n",
    "                output_dim, \n",
    "                kernel_size=wrapped_layer.kernel_size,\n",
    "                padding=wrapped_layer.padding,\n",
    "                stride=wrapped_layer.stride),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.layer = nn.Sequential(\n",
    "            wrapped_layer,\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            activation\n",
    "        ) \n",
    "\n",
    "        self.kl_loss = self.make_kl_loss(activation)\n",
    "        self.max_alpha = max_alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_out = self.layer(X)\n",
    "        alpha = self.min_alpha + self.max_alpha * self.get_alpha(X)\n",
    "        eps = self.sample_lognormal(alpha)\n",
    "        X_out_trunc = torch.where(X_out > 0, X_out, self.min_alpha * torch.ones_like(X_out))\n",
    "\n",
    "        kl_loss = self.kl_loss(torch.log(X_out_trunc), alpha)\n",
    "        if self.training:\n",
    "            X_out = eps * X_out\n",
    "        return X_out, kl_loss.mean()\n",
    "        \n",
    "    def sample_lognormal(self, sigma):\n",
    "        batch_size = sigma.size()[0]\n",
    "        shape = sigma.size()[1:]\n",
    "        zeros = to_model_dtype(self, torch.zeros(shape))\n",
    "        ones = to_model_dtype(self, torch.ones(shape))\n",
    "        gaussian = torch.distributions.Normal(zeros, ones)\n",
    "        random_normal_sample = gaussian.sample([batch_size])\n",
    "        return torch.exp(sigma * random_normal_sample)\n",
    "    \n",
    "    def make_kl_loss(self, activation):\n",
    "        if isinstance(activation, nn.Softplus):\n",
    "            def _get_kl_loss(mu, sigma):\n",
    "                self.mu1 = torch.nn.Parameter(torch.zeros([]))\n",
    "                self.sigma1 = torch.nn.Parameter(torch.ones([])) \n",
    "                sigma1 = self.sigma1\n",
    "                mu1 = self.mu1\n",
    "                kl = 0.5 * ((sigma / sigma1) ** 2 + (mu - mu1)** 2/ sigma1 ** 2 - 1 + 2 * (torch.log(sigma1) - torch.log(sigma)))\n",
    "                return kl.view(kl.size(0), -1).mean(dim=1)\n",
    "        elif isinstance(activation, nn.ReLU):\n",
    "            def _get_kl_loss(mu=None, alpha=None):\n",
    "                kl = - torch.log(alpha / (self.max_alpha + self.min_alpha))\n",
    "                return kl.view(kl.size(0), -1).mean(dim=1)\n",
    "        return _get_kl_loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = InfoDropout(nn.Conv2d(3, 3, 3, padding=1), activation=nn.ReLU())\n",
    "assert dropout(x)[0].shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fp16(model):\n",
    "    model.half().cuda()\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.BatchNorm2d):\n",
    "            layer.float()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conv_net = AllConvNet(use_information_dropout=True)\n",
    "model = all_conv_net.cuda()\n",
    "loss = torch.nn.CrossEntropyLoss()#, beta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x.cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones([2], dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x.cuda())[0]#.shape#, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignite training operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def process_function(engine, batch, model, loss, optimizer, beta):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    x = whitening_transform(x.cuda())\n",
    "    y_pred, kl_loss_value = model(x)\n",
    "    y = y.cuda()\n",
    "    loss_value = F.cross_entropy(y_pred, y)\n",
    "    total_loss_value = loss(y_pred, y) + beta * kl_loss_value\n",
    "    total_loss_value.backward()\n",
    "    optimizer.step()\n",
    "    return y_pred, y, {'loss': total_loss_value.item(), 'log_loss': loss_value.item(), 'kl_loss': kl_loss_value.item()}\n",
    "\n",
    "\n",
    "def evaluate_function(engine, batch, model, loss, beta):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch\n",
    "        x = whitening_transform(x.cuda())\n",
    "        y = y.cuda()\n",
    "        y_pred, kl_loss_value = model(x)\n",
    "        kl_loss_value = kl_loss_value.cpu()\n",
    "        log_loss_value = F.cross_entropy(y_pred, y).cpu()\n",
    "        total_loss_value = beta * kl_loss_value.numpy() + loss(y_pred, y).cpu().numpy()\n",
    "        y = y.cpu()\n",
    "        y_pred = y_pred.cpu().float()\n",
    "        kwargs = {\n",
    "            'loss': total_loss_value,\n",
    "            'kl_loss': kl_loss_value,\n",
    "            'log_loss': log_loss_value\n",
    "        }\n",
    "        return y_pred, y, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorboardX\n",
    "\n",
    "\n",
    "def print_logs(engine, evaluator, dataloader, mode, history_dict, tb_writer):\n",
    "    evaluator.run(dataloader, max_epochs=1)\n",
    "    metrics = evaluator.state.metrics\n",
    "    loss = metrics['loss']\n",
    "    log_loss = metrics['log_loss']\n",
    "    accuracy = metrics['accuracy']\n",
    "    kl_loss = metrics['kl_loss']\n",
    "    print(mode + \" Results - Epoch {}\".format(engine.state.epoch))\n",
    "    if mode == 'Validation':\n",
    "        print('Accuracy: {}'.format(accuracy))\n",
    "    print(\n",
    "        \"loss: {:.3f} log loss: {:.3f} kl_loss: {:.3f}\"\n",
    "        .format(loss, log_loss, kl_loss))\n",
    "    if mode == 'Validation':\n",
    "        print()\n",
    "    \n",
    "    for key in metrics.keys():\n",
    "        history_dict[key].append(metrics[key])\n",
    "    tb_writer.add_scalars(\n",
    "        mode,\n",
    "        {\n",
    "            \"loss\": loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"log_loss\": log_loss,\n",
    "        }, \n",
    "        engine.state.epoch)\n",
    "    tb_writer.add_scalar(mode + \"/accuracy\", accuracy, engine.state.epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_training_loop(model, loss, epochs, beta=3.0):\n",
    "    writer = tensorboardX.SummaryWriter('./logs')\n",
    "    trainer = ignite.engine.Engine(partial(process_function, model=model, loss=loss, optimizer=optimizer, beta=beta))\n",
    "    evaluator = ignite.engine.Engine(partial(evaluate_function, model=model, loss=loss, beta=beta))\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 120, 160])\n",
    "\n",
    "    Loss(nn.CrossEntropyLoss(), output_transform=lambda x: [x[0], x[1]]).attach(evaluator, 'log_loss')\n",
    "    accuracy = Accuracy(output_transform=lambda x: [x[0], x[1]])\n",
    "    accuracy.attach(trainer, 'accuracy')\n",
    "    accuracy.attach(evaluator, 'accuracy')\n",
    "\n",
    "    Loss(lambda *input, **kwargs: kwargs['kl_loss']).attach(evaluator, 'kl_loss')\n",
    "    Loss(lambda *input, **kwargs: kwargs['loss']).attach(evaluator, 'loss')\n",
    "    \n",
    "    training_history = {'log_loss': [], 'loss': [], 'kl_loss': [], 'accuracy': []}\n",
    "    validation_history = {'log_loss': [], 'loss': [], 'kl_loss': [], 'accuracy': []}\n",
    "    \n",
    "    trainer.add_event_handler(ignite.engine.Events.EPOCH_COMPLETED, print_logs, evaluator, cifar_dl_train, 'Training', training_history, writer)\n",
    "    trainer.add_event_handler(ignite.engine.Events.EPOCH_COMPLETED, print_logs, evaluator, cifar_dl_test, 'Validation', validation_history, writer)\n",
    "    trainer.add_event_handler(ignite.engine.Events.EPOCH_COMPLETED, lambda engine: scheduler.step())\n",
    "    \n",
    "    trainer.run(cifar_dl_train, max_epochs=epochs)\n",
    "    return model, evaluator, training_history, validation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.001\n",
    "beta = 3.0\n",
    "use_information_dropout = True\n",
    "use_entropy_penalization = False\n",
    "entropy_penalization_beta = 0.01\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "all_conv_net = AllConvNet(use_information_dropout=use_information_dropout)\n",
    "if use_entropy_penalization:\n",
    "    loss = EntropyPenalizedLogLoss(entropy_penalization_beta)\n",
    "else:\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "model = all_conv_net.cuda()\n",
    "model.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "run_training_loop(model, loss, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import dill\n",
    "torch.save(model, open('info_dropout_low.pkl', 'wb'), dill)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
