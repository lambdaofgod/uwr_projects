{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp segmentation_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from toolz import compose\n",
    "from tensorflow.keras import losses, metrics, layers, models\n",
    "from deeplearning_image_pixelwise import data, config\n",
    "import attr\n",
    "\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    \n",
    "\n",
    "if config.float_dtype == 'float16':\n",
    "    tf.keras.backend.set_floatx('float16')\n",
    "    tf.keras.backend.set_epsilon(1e-4)\n",
    "    policy = tensorflow.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    tensorflow.keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DATA_DIR = config.DATA_DIR\n",
    "TRAIN_DIR = config.TRAIN_DIR\n",
    "TRAIN_MASK_DIR = config.TRAIN_MASK_DIR\n",
    "VAL_DIR = config.VAL_DIR\n",
    "VAL_MASK_DIR = config.VAL_MASK_DIR\n",
    "N_CLASSES = config.N_CLASSES\n",
    "BATCH_SIZE = config.BATCH_SIZE\n",
    "IMG_HEIGHT, IMG_WIDTH = config.IMG_WIDTH, config.IMG_WIDTH\n",
    "EPOCHS = 50 \n",
    "\n",
    "buffer_size = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BASE_N_FILTERS = 8\n",
    "DROPOUT_RATE = 0.5\n",
    "ACTIVATION = 'relu'\n",
    "INITIALIZER = 'glorot_normal'\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "USE_DEV_SUBSET=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "val_dataset = data.load_dataset(VAL_DIR, VAL_MASK_DIR)\n",
    "train_dataset = data.load_dataset(TRAIN_MASK_DIR, TRAIN_MASK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def unet_forward_block(input_, n_filters, dropout_rate, activation, initializer):\n",
    "    conv_out = layers.Conv2D(n_filters, (3, 3), activation=activation, kernel_initializer=initializer, padding='same')(input_)\n",
    "    conv_out = layers.BatchNormalization()(conv_out)\n",
    "    conv_out = layers.Dropout(dropout_rate) (conv_out)\n",
    "    conv_out = layers.Conv2D(n_filters, (3, 3), activation=activation, kernel_initializer=initializer, padding='same') (conv_out)\n",
    "    conv_out = layers.BatchNormalization()(conv_out)\n",
    "    pool_out = layers.MaxPooling2D((2, 2)) (conv_out)\n",
    "    return conv_out, pool_out \n",
    "    \n",
    "    \n",
    "def unet_skip_connect_block(current, skip_connected, n_filters, dropout_rate, activation, initializer):\n",
    "    conv_current = layers.Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same') (current)\n",
    "    skip_connected_concat = layers.concatenate([conv_current, skip_connected])\n",
    "    skip_connected_concat = layers.Conv2D(2 * n_filters, (3, 3), activation=activation, kernel_initializer=initializer, padding='same') (skip_connected_concat)\n",
    "    skip_connected_concat = layers.BatchNormalization()(skip_connected_concat)\n",
    "    skip_connected_concat = layers.Dropout(dropout_rate) (skip_connected_concat)\n",
    "    skip_connected_concat = layers.Conv2D(2 * n_filters, (3, 3), activation=activation, kernel_initializer=initializer, padding='same') (skip_connected_concat)\n",
    "    return layers.BatchNormalization()(skip_connected_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric - IoU\n",
    "\n",
    "Mean Intersection over Union is commonly used for evaluating segmentation models - it calculates mean IoU score over classes (like in scikit-learn 'macro' averaging scheme).\n",
    "This makes this metric care about each class equally, and not be overpowered by classes with many pixels, what happens to accuracy.\n",
    "\n",
    "MeanIOU from tf.keras.metrics can't handle logits (it operates on labels) so there was a need to write this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def tf_casted_sum(tensor, dtype=tf.uint8):\n",
    "    return tf.math.reduce_sum(tf.cast(tensor, dtype))\n",
    "\n",
    "\n",
    "def iou(masks, masks_logits_pred, category):\n",
    "    masks_pred = tf.cast(tf.math.argmax(masks_logits_pred, axis=-1), tf.int32)\n",
    "    positive = masks == category\n",
    "    negative = masks != category\n",
    "    positive_pred = masks_pred == category\n",
    "    negative_pred = masks_pred != category\n",
    "    intersection = tf_casted_sum(\n",
    "        tf.math.logical_and(positive_pred, positive[:,:,:,0])\n",
    "    )\n",
    "    union = tf_casted_sum(positive) + tf_casted_sum(positive_pred) - intersection\n",
    "    return tf.cond(union > 0, lambda: intersection / union, lambda: tf.ones((), dtype=tf.float32))\n",
    "\n",
    "\n",
    "def mean_iou(y_true, y_pred, ignored_indices=[N_CLASSES-1]):\n",
    "    \"\"\"\n",
    "    Return the Intersection over Union (IoU) score.\n",
    "    Args:\n",
    "        y_true: the expected y values as a one-hot\n",
    "        y_pred: the predicted y values as a one-hot or softmax output\n",
    "    Returns:\n",
    "        the scalar IoU value (mean over all labels)\n",
    "    \"\"\"\n",
    "    # get number of labels to calculate IoU for\n",
    "    num_labels = y_pred.shape[-1]\n",
    "    # initialize a variable to store total IoU in\n",
    "    total_iou = 0 #tf.zeros((),)\n",
    "    # iterate over labels to calculate IoU for\n",
    "    for label in range(num_labels):\n",
    "        if label not in ignored_indices:\n",
    "            total_iou = total_iou + iou(y_true, y_pred, label)\n",
    "    # divide total IoU by number of labels to get mean IoU\n",
    "    return total_iou / (num_labels - len(ignored_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow had some weird issue with dtypes, so in case of using float16 a custom accuracy was needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def accuracy(y_true, y_pred_logits):\n",
    "    y_true = tf.cast(y_true, tf.uint8)[:,:,0]\n",
    "    y_pred = tf.cast(tf.math.argmax(y_pred_logits, axis=-1), tf.uint8)\n",
    "    return tf.math.reduce_mean(tf.cast(y_pred == y_true, config.float_dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_default_callbacks():\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='weights.{epoch:02d}-{mean_iou:.3f}.hdf5',\n",
    "        monitor='mean_iou'\n",
    "    )\n",
    "    # this ridiculous profile_batch parameter is passed because if it's not set then Tensorboard complaints about seting trace\n",
    "    tensorboard_training_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='.logs', histogram_freq=0, write_images=False,\n",
    "        update_freq=100,\n",
    "        profile_batch=100000000\n",
    "    )\n",
    "    tensorboard_epoch_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='.logs', histogram_freq=0, write_images=False,\n",
    "        update_freq='epoch', profile_batch=100000000\n",
    "    )\n",
    "    return [model_checkpoint_callback, tensorboard_training_callback, tensorboard_epoch_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_segmentation_model(\n",
    "        input_shape,\n",
    "        n_classes,\n",
    "        base_n_filters=BASE_N_FILTERS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        activation=ACTIVATION,\n",
    "        initializer=INITIALIZER\n",
    "    ):\n",
    "    # Build U-Net segmentation_model\n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    c1, p1 = unet_forward_block(inputs, 2 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    c2, p2 = unet_forward_block(p1, 4 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    c3, p3 = unet_forward_block(p2, 8 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    c4, p4 = unet_forward_block(p3, 8 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    c5, __ = unet_forward_block(p4, 16 * base_n_filters, dropout_rate, activation, initializer)\n",
    "\n",
    "    #concating starts\n",
    "    u6 = unet_skip_connect_block(c5, c4, 8 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    u7 = unet_skip_connect_block(u6, c3, 4 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    u8 = unet_skip_connect_block(u7, c2, 2 * base_n_filters, dropout_rate, activation, initializer)\n",
    "    u9 = unet_skip_connect_block(u8, c1, base_n_filters, dropout_rate, activation, initializer)\n",
    "\n",
    "    out = layers.Conv2D(n_classes, (1, 1)) (u9)\n",
    "    # for some reason SparseCategoricalCrossEntropy fails if the output is fp16\n",
    "    out = tf.cast(out, tf.float32)\n",
    "    return models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "\n",
    "def setup_segmentation_model(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        n_classes=N_CLASSES,\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tensorflow.keras.optimizers.Adam(LEARNING_RATE),\n",
    "        metrics=[accuracy, mean_iou]\n",
    "    ):\n",
    "    segmentation_model = build_segmentation_model(input_shape, n_classes)\n",
    "    segmentation_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return segmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "segmentation_model = setup_segmentation_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(segmentation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "if USE_DEV_SUBSET:\n",
    "    train_dataset = train_dataset.take(2 ** 14)\n",
    "    val_dataset = val_dataset.take(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    segmentation_model.fit(\n",
    "        train_dataset.batch(BATCH_SIZE).shuffle(buffer_size).repeat(), \n",
    "        validation_data=val_dataset.batch(BATCH_SIZE),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=get_default_callbacks(),\n",
    "        steps_per_epoch=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
